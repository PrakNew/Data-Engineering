{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following commands to load the data\n",
    "# wget https://police-incidents-dataset-ds.s3.amazonaws.com/Police_Department_Incident_Reports__2018_to_Present.csv\n",
    "\n",
    "# Then put the data into the hadoop livy folder \n",
    "# hadoop fs -put Police_Department_Incident_Reports__2018_to_Present.csv /user/livy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1 = spark.read.csv(\"Police_Department_Incident_Reports__2018_to_Present.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fileSchema = StructType([StructField('Incident_DateTime', StringType(),True),\n",
    "                        StructField('Incident_Date', StringType(),True),\n",
    "                        StructField('Incident_Time', StringType(),True),\n",
    "                        StructField('Incident_Year', IntegerType(),True),\n",
    "                        StructField('Incident_DayOfWeek', StringType(),True),\n",
    "                        StructField('Report_DateTime', StringType(),True),\n",
    "                        StructField('Row_Id', LongType(),True),\n",
    "                        StructField('Incident_Id', IntegerType(),True),\n",
    "                        StructField('Incident_Number', IntegerType(),True),\n",
    "                        StructField('Cad_Number', IntegerType(),True),\n",
    "                        StructField('Report_Type_Code', StringType(),True),\n",
    "                        StructField('Report_Type_Description', StringType(),True),\n",
    "                        StructField('File_Online', BooleanType(),True),\n",
    "                        StructField('Incident_Code', IntegerType(),True),\n",
    "                        StructField('Incident_Category', StringType(),True),\n",
    "                        StructField('Incident_Subcategory', StringType(),True),\n",
    "                        StructField('Incident_Description', StringType(),True), \n",
    "                        StructField('Resolution', StringType(),True),\n",
    "                        StructField('Intersection', StringType(),True),\n",
    "                        StructField('CNN', DoubleType(),True),\n",
    "                        StructField('Police_District', StringType(),True),\n",
    "                        StructField('Analysis_Neighbourhood', StringType(),True),\n",
    "                        StructField('Supervisor_District', IntegerType(),True),\n",
    "                        StructField('Latitude', DoubleType(),True), \n",
    "                        StructField('Longitide', DoubleType(),True),\n",
    "                        StructField('Point', StringType(),True), \n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1 = spark.read.csv(\"Police_Department_Incident_Reports__2018_to_Present.csv\", header = True, schema = fileSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1.select('Incident_id', 'Incident_Category').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1.select('Incident_Category').distinct().show(truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1.select('Incident_Category').groupBy('Incident_Category').count().orderBy(\"count\", ascending = False).show(52, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyzing datetime columns in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1.select(\"Incident_DateTime\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern1 = 'yyyy/MM/dd hh:mm:ss aa'\n",
    "file2 = file1.withColumn('Incident_DateTime', unix_timestamp(file1['Incident_DateTime'], pattern1).cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2.select(year('Incident_DateTime')).distinct().show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1.select('Incident_DateTime', 'Incident_Date','Incident_Time', 'Incident_Year', 'Report_DateTime').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern1 = 'yyyy/MM/dd hh:mm:ss aa'\n",
    "pattern2 = 'yyyy/MM/dd'\n",
    "pattern3 = 'hh:mm'\n",
    "pattern4 = 'yyyy'\n",
    "file2 = file1.withColumn('Incident_DateTime', unix_timestamp(file1['Incident_DateTime'], pattern1).cast('timestamp'))\\\n",
    ".withColumn('Incident_Date', unix_timestamp(file1['Incident_Date'], pattern2).cast('timestamp'))\\\n",
    ".withColumn('Incident_Time', unix_timestamp(file1['Incident_Time'], pattern3).cast('timestamp'))\\\n",
    ".withColumn('Report_DateTime', unix_timestamp(file1['Report_DateTime'], pattern1).cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####   Analysis 1 ##############\n",
    "# Find the days of the week on which maximum incidents has happened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "file2.select(dayofweek(\"Incident_DateTime\")).show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2.select(date_format(\"Incident_DateTime\",'E')).show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding a new column in our dataframe,which add the day of the week in each record\n",
    "file3 =file2.withColumn('dayOfTheWeek' , date_format(\"Incident_DateTime\",'E'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aggregating based on the day of the week -- this will get us the day of the week, on which maximum incidents happened \n",
    "file3.groupBy('dayOfTheWeek').count().orderBy('count', ascending = False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################  Analysis 2    ####################\n",
    "\n",
    "# What percent of the incidents has been recorded online\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " file2.select(\"File_Online\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file3 = file2.withColumn(\"File_Online\" ,when(col(\"File_Online\") == True , True).otherwise(False) )\n",
    "file3.select('File_Online').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file4 = file3.select(\"File_Online\").groupBy('File_Online').count()\n",
    "\n",
    "file4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "file4.withColumn( 'colnew' ,col('count') / sum('count').over(Window.partitionBy())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "############   Analysis 3 ###################\n",
    "\n",
    "# Group by the numbers of incidents reported based on each Year\n",
    "\n",
    "incidents_reporter_per_year = file2.select(year('Incident_DateTime')).groupBy('year(Incident_DateTime)').count()\n",
    "\n",
    "incidents_reporter_per_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Running SQL queries in spark ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### creating a temporary table ######\n",
    "file2.registerTempTable(\"police_report_data\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from police_report_data\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Finding the number of incidents of for each incident_category\n",
    "spark.sql(\"select Incident_Category , count(Incident_Category) from police_report_data group by  Incident_Category\").show(52, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####   Analysis 2 ##############\n",
    "# Find the days of the week on which maximum incidents has happened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2.withColumn('dayOfTheWeek' , date_format(\"Incident_DateTime\",'E')).registerTempTable(\"police_report_data_with_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql('select dayOfTheWeek from police_report_data_with_day ').show(12, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql('select dayOfTheWeek , count(dayOfTheWeek) from police_report_data_with_day group by dayOfTheWeek order by count(dayOfTheWeek) desc ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################  Analysis 3    ####################\n",
    "\n",
    "# What percent of the incidents has been recorded online\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# file2.select(\"File_Online\").show()\n",
    "spark.sql(\"select File_Online from police_report_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#file3 = file2.withColumn(\"File_Online\" ,when(col(\"File_Online\") == True , True).otherwise(False) )\n",
    "#update users set name = '*' where name is null\n",
    "spark.sql(\"select  ((count(*) - count(File_Online))/count(*))*100 as offline_percent  , \\\n",
    "(100 - ((count(*) - count(File_Online))/count(*))*100) as online_percent from police_report_data \").\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############   Analysis 3 ###################\n",
    "\n",
    "# Group by the numbers of incidents reported based on each Year\n",
    "\n",
    "spark.sql(\"select  year(Incident_DateTime) as year ,  count(*) as no_incidents from police_report_data group by \\\n",
    "year(Incident_DateTime)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########### How many cases of Assault happened on particular month say in Jan 2020  ############\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select  year(Incident_DateTime) as year , month (Incident_DateTime) as month,  count(*) \\\n",
    "as no_incidents from police_report_data where year(Incident_DateTime)= 2020 and month (Incident_DateTime) = 3 group by year, month \").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extras ------------------------------------------------>\n",
    "\n",
    "Let us look at the queries used in this video:\n",
    "\n",
    "\n",
    "Analysis 1:\n",
    "\n",
    "Find the days of the week on which maximum incidents have happened.\n",
    "\n",
    " \n",
    "\n",
    "We already have a column named 'day of week' in our dataset. But let us use Incident_DateTime column for our query.\n",
    "\n",
    " \n",
    "\n",
    "file2.select(dayofweek(\"Incident_DateTime\")).show(7)\n",
    "Since the data type of Incident_DateTime column is timestamp, we can use the day of week() method on it. it will give us the day of the week as numbers for each incident.\n",
    "\n",
    " \n",
    "\n",
    "But for our analysis, we need dayofweek as string. For this, we use the following code:\n",
    "\n",
    "file2.select(date_format(\"Incident_DateTime\",'E')).show(7)\n",
    " \n",
    "\n",
    "'E' here is used to get the string format day of the week from the column \"Incident_DateTime\". To understand this date_format method, you can use the link in the additional readings.\n",
    "\n",
    " \n",
    "\n",
    "So, we know how we can find dayofweek in string format from timestamp \"Incident_DateTime\". Let us add a new column named \"dayoftheweek\" in our dataset. Since dataframes are immutable, we add this column to our 'file2' dataframe and store the result in a new dataframe 'file3'.\n",
    "\n",
    " \n",
    "\n",
    "# Adding a new column in our dataframe,which add the day of the week in each record\n",
    "file3 =file2.withColumn('dayOfTheWeek' , date_format(\"Incident_DateTime\",'E'))\n",
    " \n",
    "\n",
    "To run the analysis, we use the following query,\n",
    "\n",
    "# Aggregating based on the day of the week -- this will get us the day of the week, on which maximum incidents happened \n",
    "file3.groupBy('dayOfTheWeek').count().orderBy('count', ascending = False).show()\n",
    " \n",
    "\n",
    "Code Description:\n",
    "\n",
    "We groupBy on the dayOfTheWeek column and count the number of rows in each group. We, then, arrange the count column that is created by the query to show the day when most incidents happened.\n",
    "\n",
    " \n",
    "\n",
    "Analysis 2:\n",
    "\n",
    "What percent of the incidents have been recorded online?\n",
    "\n",
    "We are using file2 that contains DateTime columns in timestamp data type.\n",
    "\n",
    "Let us first analyze this column \"File_Online\" using the following code.\n",
    "\n",
    "file2.select(\"File_Online\").show()\n",
    " \n",
    "\n",
    "As discussed in the video, this is a boolean data type column that contains 'true' for cases reported online and 'null' for cases reported offline. We can find the percent of incidents recorded online with this column as well. But let us see how we can update this column values.\n",
    "\n",
    "file3 = file2.withColumn(\"File_Online\" ,when(col(\"File_Online\") == True , True).otherwise(False) )\n",
    "file3.select('File_Online').show()\n",
    " \n",
    "\n",
    "Code Description:\n",
    "\n",
    "Using withColumn(), we have specified that if the value of \"File_Online\" is True, it should remain True, but in any other case, it should be 'False'. Also, as dataframes are immutable, we have stored the result in a new dataframe 'file3'.\n",
    "\n",
    " \n",
    "\n",
    "Now, let us start the analysis of incidents reported online. we first create a new dataframe from 'file3' that contains the \"File_Online\" column along with the count of incidents recorded online as well as offline. This new dataframe is stored in 'file4' using the following code.\n",
    "\n",
    "file4 = file3.select(\"File_Online\").groupBy('File_Online').count()\n",
    "\n",
    "file4.show()\n",
    " \n",
    "\n",
    "Since we have to find the percent, we have to sum all the incidents reported. To do that, we use a function 'over(Window.partitionBy())'. \n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "file4.withColumn( 'colnew' ,col('count') / sum('count').over(Window.partitionBy())).show()\n",
    "\n",
    "Analysis 3:\n",
    "\n",
    "Find the number of incidents reported each year in our dataset.\n",
    "\n",
    "# Group by the numbers of incidents reported based on each Year\n",
    "\n",
    "incidents_reporter_per_year = file2.select(year('Incident_DateTime')).groupBy('year(Incident_DateTime)').count()\n",
    "\n",
    "incidents_reporter_per_year.show()\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "Some of the other operations that are available on dataframes include:\n",
    "\n",
    "Union of two Dataframes\n",
    "\n",
    "unionDF = df1.unionAll(df2)\n",
    " \n",
    "\n",
    "Filter based on the value\n",
    "\n",
    "explodeDF.filter(explodeDF.firstName == \"xiangrui\").sort(explodeDF.lastName)\n",
    "\n",
    "\n",
    "\n",
    "filterDF = explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    " \n",
    "\n",
    "Treatment of null rows \n",
    "\n",
    "#Drop records with null value\n",
    "\n",
    "dropNullDF = explodeDF.na.drop()\n",
    " \n",
    "\n",
    "Aggregate function\n",
    "\n",
    "salarySumDF = explodeDF.agg({\"salary\" : \"sum\"})\n",
    " \n",
    "\n",
    "This segment was filled with code demos that included various queries on a police dataset. In the upcoming segments, we will understand how we can run the same queries using Spark SQL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
